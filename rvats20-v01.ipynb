{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":9102725,"sourceType":"datasetVersion","datasetId":5493674},{"sourceId":9107824,"sourceType":"datasetVersion","datasetId":5496762},{"sourceId":9107963,"sourceType":"datasetVersion","datasetId":5496847},{"sourceId":9108069,"sourceType":"datasetVersion","datasetId":5496920}],"dockerImageVersionId":30747,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Packages","metadata":{}},{"cell_type":"code","source":"%pip install /kaggle/input/lmsys-packages/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2024-11-26T20:00:40.717544Z","iopub.execute_input":"2024-11-26T20:00:40.717820Z","iopub.status.idle":"2024-11-26T20:01:27.274043Z","shell.execute_reply.started":"2024-11-26T20:00:40.717798Z","shell.execute_reply":"2024-11-26T20:01:27.272882Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/lmsys-packages/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from triton==2.2.0) (3.13.1)\nInstalling collected packages: triton\nSuccessfully installed triton-2.2.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%pip install /kaggle/input/lmsys-packages/xformers-0.0.24042abc8.d20240802-cp310-cp310-linux_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2024-11-26T20:01:27.275984Z","iopub.execute_input":"2024-11-26T20:01:27.276290Z","iopub.status.idle":"2024-11-26T20:02:16.117392Z","shell.execute_reply.started":"2024-11-26T20:01:27.276266Z","shell.execute_reply":"2024-11-26T20:02:16.116312Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/lmsys-packages/xformers-0.0.24042abc8.d20240802-cp310-cp310-linux_x86_64.whl\nRequirement already satisfied: torch>=1.12 in /opt/conda/lib/python3.10/site-packages (from xformers==0.0.24042abc8.d20240802) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xformers==0.0.24042abc8.d20240802) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (2024.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.12->xformers==0.0.24042abc8.d20240802) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.12->xformers==0.0.24042abc8.d20240802) (1.3.0)\nInstalling collected packages: xformers\nSuccessfully installed xformers-0.0.24+042abc8.d20240802\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!cp -r /kaggle/input/lmsys-modules-0805 human_pref","metadata":{"execution":{"iopub.status.busy":"2024-11-26T20:02:16.119004Z","iopub.execute_input":"2024-11-26T20:02:16.119390Z","iopub.status.idle":"2024-11-26T20:02:17.183192Z","shell.execute_reply.started":"2024-11-26T20:02:16.119354Z","shell.execute_reply":"2024-11-26T20:02:17.182135Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Prepare test file","metadata":{}},{"cell_type":"code","source":"%%writefile prepare_test_file.py\nimport pandas as pd\n\n\ndf = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\ndf[\"winner_model_a\"] = 1\ndf[\"winner_model_b\"] = 0\ndf[\"winner_tie\"] = 0\ndf.to_parquet(\"test.parquet\", index=False)\n\ndf[\"response_a\"], df[\"response_b\"] = df[\"response_b\"], df[\"response_a\"]\ndf.to_parquet(\"test_swap.parquet\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-26T20:02:17.186247Z","iopub.execute_input":"2024-11-26T20:02:17.186903Z","iopub.status.idle":"2024-11-26T20:02:17.192989Z","shell.execute_reply.started":"2024-11-26T20:02:17.186875Z","shell.execute_reply":"2024-11-26T20:02:17.192253Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Writing prepare_test_file.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!python prepare_test_file.py","metadata":{"execution":{"iopub.status.busy":"2024-11-26T20:02:17.194101Z","iopub.execute_input":"2024-11-26T20:02:17.194345Z","iopub.status.idle":"2024-11-26T20:02:18.926269Z","shell.execute_reply.started":"2024-11-26T20:02:17.194325Z","shell.execute_reply":"2024-11-26T20:02:18.925109Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Inference: gemma2-9b","metadata":{}},{"cell_type":"code","source":"%%writefile predict_m0.py\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\n# from xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\nfrom human_pref.models.modeling_gemma2 import Gemma2ForSequenceClassification\nfrom human_pref.data.processors import ProcessorPAB\nfrom human_pref.data.dataset import LMSYSDataset\nfrom human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\nfrom human_pref.utils import to_device\n\n\nmodel_name_or_path = \"/kaggle/input/lmsys-checkpoints-0-0805\"\ncsv_path = \"test.parquet\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\nprocessor = ProcessorPAB(\n    tokenizer=tokenizer,\n    max_length=4096,\n    support_system_role=False,\n)\ndataset = LMSYSDataset(\n    csv_file=csv_path,\n    query=None,\n    processor=processor,\n    include_swap=False,\n    is_parquet=True,\n)\ndataloader = DataLoader(\n    dataset,\n    batch_size=80,\n    num_workers=4,\n    collate_fn=ShardedMaxTokensCollator(\n        max_tokens=8192, base_collator=VarlenCollator()\n    ),\n)\n\n# model for pipelined inference\nnum_hidden_layers = 42\ndevice_map = {\n    \"model.embed_tokens\": \"cuda:0\",\n    \"model.norm\": \"cuda:1\",\n    \"score\": \"cuda:1\",\n}\nfor i in range(num_hidden_layers // 2):\n    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\nfor i in range(num_hidden_layers // 2, num_hidden_layers):\n    device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n\nmodel = Gemma2ForSequenceClassification.from_pretrained(\n    model_name_or_path,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\n\n# inv_freq clones for each device\nconfig = model.config\ndim = config.head_dim\ninv_freq = 1.0 / (\n    config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n)\ninv_freq0 = inv_freq.to(\"cuda:0\")\ninv_freq1 = inv_freq.to(\"cuda:1\")\n\n\n# for name, p in model.named_parameters():\n#     print(name, p.device)\n# for name, b in model.model.named_buffers():\n#     print(name, b.device)\n\n# pipeline parallelism with two GPUs\nis_first = True\nhidden_states = None\nouts = []\nfor batch in tqdm(dataloader):\n    for micro_batch in batch:\n        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n        seq_info = dict(\n            cu_seqlens=micro_batch[\"cu_seqlens\"],\n            position_ids=micro_batch[\"position_ids\"],\n            max_seq_len=micro_batch[\"max_seq_len\"],\n            # attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n        )\n        seq_info = to_device(seq_info, \"cuda:0\")\n        if is_first:\n            with torch.no_grad(), torch.cuda.amp.autocast():\n                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n            is_first = False\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, prev_hidden_states], \"cuda:1\"\n            )\n            continue\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, hidden_states], \"cuda:1\"\n            )\n            outs.append(logits.cpu())\n\n# last micro-batch\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n    outs.append(logits.cpu())\n\npred = torch.cat(outs, dim=0)\nprob = pred.softmax(-1)\nprint(dataset.evaluate(prob.numpy()))\n\nnp.save('prob_m0.npy', prob)","metadata":{"execution":{"iopub.status.busy":"2024-11-26T20:02:18.927945Z","iopub.execute_input":"2024-11-26T20:02:18.928242Z","iopub.status.idle":"2024-11-26T20:02:18.935430Z","shell.execute_reply.started":"2024-11-26T20:02:18.928218Z","shell.execute_reply":"2024-11-26T20:02:18.934550Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Writing predict_m0.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!python predict_m0.py","metadata":{"execution":{"iopub.status.busy":"2024-11-26T20:02:18.936587Z","iopub.execute_input":"2024-11-26T20:02:18.936855Z","iopub.status.idle":"2024-11-26T20:06:05.757312Z","shell.execute_reply.started":"2024-11-26T20:02:18.936834Z","shell.execute_reply":"2024-11-26T20:06:05.756217Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Loading checkpoint shards: 100%|██████████████████| 4/4 [03:19<00:00, 49.80s/it]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]2024-11-26 20:05:47.706805: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-11-26 20:05:47.706940: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-11-26 20:05:47.837094: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n100%|█████████████████████████████████████████████| 1/1 [00:15<00:00, 15.67s/it]\n{'log_loss': 3.0968185681481395}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Inference: llama3-8b","metadata":{}},{"cell_type":"code","source":"%%writefile predict_m3.py\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\nfrom xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\nfrom human_pref.models.modeling_llama import LlamaForSequenceClassification\nfrom human_pref.data.processors import ProcessorPAB\nfrom human_pref.data.dataset import LMSYSDataset\nfrom human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\nfrom human_pref.utils import to_device\n\n\nmodel_name_or_path = \"/kaggle/input/lmsys-checkpoints-3-0805\"\ncsv_path = \"test_swap.parquet\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\ntokenizer.deprecation_warnings[\n    \"sequence-length-is-longer-than-the-specified-maximum\"\n] = True\nprocessor = ProcessorPAB(\n    tokenizer=tokenizer,\n    max_length=4096,\n    support_system_role=True,\n)\ndataset = LMSYSDataset(\n    csv_file=csv_path,\n    query=None,\n    processor=processor,\n    include_swap=False,\n    is_parquet=True,\n)\ndataloader = DataLoader(\n    dataset,\n    batch_size=80,\n    num_workers=4,\n    collate_fn=ShardedMaxTokensCollator(\n        max_tokens=8192, base_collator=VarlenCollator()\n    ),\n)\n\n# model for pipelined inference\nnum_hidden_layers = 32\ndevice_map = {\n    \"model.embed_tokens\": \"cuda:0\",\n    \"model.norm\": \"cuda:1\",\n    \"score\": \"cuda:1\",\n}\nfor i in range(num_hidden_layers // 2):\n    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\nfor i in range(num_hidden_layers // 2, num_hidden_layers):\n    device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n\nmodel = LlamaForSequenceClassification.from_pretrained(\n    model_name_or_path,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\n\n# inv_freq clones for each device\nconfig = model.config\ndim = config.hidden_size // config.num_attention_heads\ninv_freq = 1.0 / (\n    config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n)\ninv_freq0 = inv_freq.to(\"cuda:0\")\ninv_freq1 = inv_freq.to(\"cuda:1\")\n\n\n# for name, p in model.named_parameters():\n#     print(name, p.device)\n# for name, b in model.model.named_buffers():\n#     print(name, b.device)\n\n# pipeline parallelism with two GPUs\nis_first = True\nhidden_states = None\nouts = []\nfor batch in tqdm(dataloader):\n    for micro_batch in batch:\n        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n        seq_info = dict(\n            cu_seqlens=micro_batch[\"cu_seqlens\"],\n            position_ids=micro_batch[\"position_ids\"],\n            max_seq_len=micro_batch[\"max_seq_len\"],\n            attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n        )\n        seq_info = to_device(seq_info, \"cuda:0\")\n        if is_first:\n            with torch.no_grad(), torch.cuda.amp.autocast():\n                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n            is_first = False\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, prev_hidden_states], \"cuda:1\"\n            )\n            continue\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, hidden_states], \"cuda:1\"\n            )\n            outs.append(logits.cpu())\n\n# last micro-batch\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n    outs.append(logits.cpu())\n\n\npred = torch.cat(outs, dim=0)\nprob = pred.softmax(-1)\nprint(dataset.evaluate(prob.numpy()))\n\nnp.save('prob_m3.npy', prob)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:06:05.758785Z","iopub.execute_input":"2024-11-26T20:06:05.759090Z","iopub.status.idle":"2024-11-26T20:06:05.766991Z","shell.execute_reply.started":"2024-11-26T20:06:05.759065Z","shell.execute_reply":"2024-11-26T20:06:05.765949Z"}},"outputs":[{"name":"stdout","text":"Writing predict_m3.py\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!python predict_m3.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:06:05.767957Z","iopub.execute_input":"2024-11-26T20:06:05.768328Z","iopub.status.idle":"2024-11-26T20:09:02.418630Z","shell.execute_reply.started":"2024-11-26T20:06:05.768307Z","shell.execute_reply":"2024-11-26T20:09:02.417307Z"}},"outputs":[{"name":"stdout","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nLoading checkpoint shards: 100%|██████████████████| 4/4 [02:42<00:00, 40.70s/it]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]2024-11-26 20:08:53.970247: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-11-26 20:08:53.970329: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-11-26 20:08:53.972091: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.86s/it]\n{'log_loss': 0.7582519183970864}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Make submission","metadata":{}},{"cell_type":"code","source":"%%writefile make_submission.py\nimport numpy as np\nimport pandas as pd\n\ndf = pd.read_parquet(\"test.parquet\")\npreds = np.average(\n    [\n        np.load(\"prob_m0.npy\"),\n        np.load(\"prob_m3.npy\")[:, [1, 0, 2]],\n    ],\n    axis=0,\n    weights=[2, 1],\n)\nsub = pd.DataFrame({\n    \"id\": df[\"id\"],\n    \"winner_model_a\": preds[:, 0],\n    \"winner_model_b\": preds[:, 1],\n    \"winner_tie\": preds[:, 2],\n})\nsub.to_csv(\"submission.csv\", index=False)\nprint(sub.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:09:02.421749Z","iopub.execute_input":"2024-11-26T20:09:02.422066Z","iopub.status.idle":"2024-11-26T20:09:02.427948Z","shell.execute_reply.started":"2024-11-26T20:09:02.422027Z","shell.execute_reply":"2024-11-26T20:09:02.427127Z"}},"outputs":[{"name":"stdout","text":"Writing make_submission.py\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!python make_submission.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:09:02.428869Z","iopub.execute_input":"2024-11-26T20:09:02.429127Z","iopub.status.idle":"2024-11-26T20:09:04.335967Z","shell.execute_reply.started":"2024-11-26T20:09:02.429108Z","shell.execute_reply":"2024-11-26T20:09:04.335088Z"}},"outputs":[{"name":"stdout","text":"        id  winner_model_a  winner_model_b  winner_tie\n0   136060        0.002276        0.981058    0.016666\n1   211333        0.515450        0.128106    0.356444\n2  1233961        0.084381        0.765794    0.149825\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}